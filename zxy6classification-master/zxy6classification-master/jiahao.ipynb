{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from pyglet.window import key\n",
    "import pickle, os, gzip\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from gym.envs.box2d.car_racing import CarRacing\n",
    "from skimage.color import rgb2gray\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Classification:\n",
    "    actions = np.array([\n",
    "    [ 0.0, 0.0, 0.0],  # STRAIGHT\n",
    "    [ 0.0, 1.0, 0.0],  # ACCELERATE\n",
    "    [ 1.0, 0.0, 0.0],  # RIGHT\n",
    "    [ 1.0, 0.0, 0.4],  # RIGHT_BRAKE\n",
    "    [ 0.0, 0.0, 0.4],  # BRAKE\n",
    "    [-1.0, 0.0, 0.4],  # LEFT_BRAKE\n",
    "    [-1.0, 0.0, 0.0],  # LEFT\n",
    "    ], dtype=np.float32)\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    def action_arr2id(self,arr):\n",
    "        \"\"\" Converts action from the array format to an id (ranging from 0 to n_actions) \"\"\"\n",
    "        ids = []\n",
    "        for a in arr:\n",
    "            id = np.where(np.all(self.actions==a, axis=1))\n",
    "            ids.append(id[0][0])\n",
    "        return np.array(ids)\n",
    "\n",
    "    def action_id2arr(self, ids):\n",
    "        \"\"\" Converts action from id to array format (as understood by the environment) \"\"\"\n",
    "        return self.actions[ids.astype(int)]\n",
    "\n",
    "    def one_hot(self,labels):\n",
    "        \"\"\" One hot encodes a set of actions \"\"\"\n",
    "        one_hot_labels = np.zeros(labels.shape + (self.n_actions,))\n",
    "        for c in range(self.n_actions):\n",
    "            one_hot_labels[labels == c, c] = 1.0\n",
    "        return one_hot_labels  \n",
    "\n",
    "    def unhot(self,one_hot_labels):\n",
    "        \"\"\" One hot DEcodes a set of actions \"\"\"\n",
    "        return np.argmax(one_hot_labels, axis=1)\n",
    "\n",
    "    def transl_action_env2agent(self, acts):\n",
    "        \"\"\" Translate actions from environment's format to agent's format \"\"\"\n",
    "        act_ids = self.action_arr2id(acts)\n",
    "        return self.one_hot(act_ids)\n",
    "\n",
    "    def transl_action_agent2env(self, one_hot_labels):\n",
    "        \"\"\" Translate actions from agent's format to environment's format \"\"\"\n",
    "        ids = self.unhot(one_hot_labels)\n",
    "        return self.action_id2arr(ids)\n",
    "\n",
    "    def delete_invalid_actions(y):\n",
    "        \"\"\" Check if there is any forbidden actions in the expert database \"\"\"\n",
    "        inval_actions = [\n",
    "            [0.0, 1.0, 0.4],  # ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.4],  # RIGHT_ACCEL_BRAKE\n",
    "            [-1.0, 1.0, 0.4],  # LEFT_ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.0],  # RIGHT_ACCEL\n",
    "            [-1.0, 1.0, 0.0],  # LEFT_ACCEL\n",
    "        ]\n",
    "        y = np.array(y) # Convert to NumPy array\n",
    "        ia_count = 0\n",
    "        ia_indices = []\n",
    "        for ia in inval_actions:\n",
    "            ia_indices += list(np.where(np.all(y == ia, axis=1))[0])    \n",
    "        ia_count += len(ia_indices)\n",
    "        if ia_count > 0:\n",
    "            print(f'Removing {ia_count} invalid actions at indices: {ia_indices}')\n",
    "            y = np.delete(y, ia_indices, axis=0)\n",
    "        #return a clean action and dirty index\n",
    "        return y, ia_indices\n",
    "\n",
    "    \n",
    "class User_Input:\n",
    "    # car\n",
    "    steering = 0\n",
    "    gas = 0\n",
    "    breaking = 0\n",
    "    action = np.zeros(3, dtype=np.float32)\n",
    "    # simulation\n",
    "    escape = False\n",
    "    record = False\n",
    "    save = False\n",
    "    reset = False\n",
    "    # key press\n",
    "    up = False\n",
    "    down = False\n",
    "    right = False\n",
    "    left = False\n",
    "\n",
    "    # setter from key press\n",
    "    def on_key_press(self, k, mod):\n",
    "        # car\n",
    "        if k == key.UP:    self.up = True; return\n",
    "        if k == key.LEFT:  self.left = True; return\n",
    "        if k == key.RIGHT: self.right = True; return\n",
    "        if k == key.DOWN:  self.down = True; return\n",
    "\n",
    "        # simulation\n",
    "        if k == key.SPACE: self.reset = True; return\n",
    "        if k == key.ESCAPE:self.escape = True; return\n",
    "        if k == key.R and self.record == False:  \n",
    "            self.record = True\n",
    "            print(\"start recording\")\n",
    "            return\n",
    "        if k == key.R and self.record == True:   \n",
    "            self.record = False\n",
    "            self.save = True\n",
    "            print(\"paused recording, start saving\")\n",
    "            return\n",
    "\n",
    "    def on_key_release(self, k, mod):\n",
    "        # car\n",
    "        if k == key.UP:    self.up = False; return\n",
    "        if k == key.LEFT:  self.left = False; return\n",
    "        if k == key.RIGHT: self.right = False; return\n",
    "        if k == key.DOWN:  self.down = False; return\n",
    "\n",
    "    def check_arrow_keys(self):\n",
    "        \n",
    "        if (self.left):  \n",
    "            self.steering = -1\n",
    "        if (self.right): \n",
    "            self.steering = 1\n",
    "        if (self.up):    \n",
    "            self.gas = 1\n",
    "        if (self.down):  \n",
    "            self.breaking = 0.4\n",
    "\n",
    "        if ((not self.left) and (not self.right)): self.steering = 0\n",
    "        if (not self.up): self.gas = 0\n",
    "        if (not self.down): self.breaking = 0 \n",
    "        return\n",
    "        \n",
    "\n",
    "    # setter from action\n",
    "    def on_save(self):\n",
    "        self.save = False\n",
    "\n",
    "    # getter\n",
    "    def get_action(self):\n",
    "        self.action[0] = self.steering\n",
    "        self.action[1] = self.gas\n",
    "        self.action[2] = self.breaking\n",
    "        return self.action\n",
    "    def escape_pressed(self):\n",
    "        return self.escape\n",
    "    def record_pressed(self):\n",
    "        return self.record\n",
    "    def save_pressed(self):\n",
    "        return self.save\n",
    "    def reset_pressed(self):\n",
    "        return self.reset\n",
    "\n",
    "    \n",
    "class Data:\n",
    "    data = {\n",
    "        \"state\": [],\n",
    "        \"info\" : [],\n",
    "        \"action\": []\n",
    "        }\n",
    "    dir_folder = \"./data2\"\n",
    "\n",
    "    def record(self, current_state, info, action):\n",
    "        self.data[\"state\"].append(copy.copy(current_state))\n",
    "        self.data[\"info\"].append(info)\n",
    "        self.data[\"action\"].append(copy.copy(action))\n",
    "    \n",
    "    def vstack(self, arr):\n",
    "        stack = np.array(arr[0], dtype=np.float32)\n",
    "        for i in range(1, len(arr)):\n",
    "            stack = np.vstack((stack, arr[i]))\n",
    "        return stack\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Reads the states and actions recorded in all files inside the given directory\"\"\"\n",
    "\n",
    "        # directory_path = os.path.join(os.getcwd(), \"data2\")\n",
    "        directory_path = \"./data2\"\n",
    "        print(\"Reading data from...\" + directory_path)\n",
    "        \n",
    "        #initialise the list\n",
    "        state = []\n",
    "        action = []\n",
    "        info = []\n",
    "        \n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".pkl.gzip\"):\n",
    "                with gzip.open(os.path.join(directory_path, filename), 'rb') as f:\n",
    "                    print(filename + \" loaded\")\n",
    "                    data = pickle.load(f)\n",
    "                    #append array into list\n",
    "                    state.append(data[\"state\"])\n",
    "                    action.append(data[\"action\"])\n",
    "                    info.append(data[\"info\"])\n",
    "            \n",
    "        s = self.vstack(state)\n",
    "        a = self.vstack(action)\n",
    "        i = self.vstack(info)\n",
    "        \n",
    "        print(\"All files added\")\n",
    "        print(s.shape)\n",
    "        print(a.shape)\n",
    "        print(i.shape)\n",
    "        return s, a, i\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        # create folder if doesn't already exist\n",
    "        if not os.path.exists(self.dir_folder):\n",
    "            os.mkdir(self.dir_folder)\n",
    "\n",
    "        # get file name\n",
    "        file_name = str(datetime.now().strftime(\"%Y_%m_%d_%H-%M-%S\"))+\".pkl.gzip\"\n",
    "        dir_file = os.path.join(self.dir_folder, file_name)\n",
    "\n",
    "        # save\n",
    "        self.data[\"state\"]=Data.preprocess_state(self.data[\"state\"])\n",
    "        print(\"image processed\")\n",
    "        f = gzip.open(dir_file, 'wb')\n",
    "        pickle.dump(self.data, f)\n",
    "        print(\"saved data to \"+dir_file)\n",
    "\n",
    "        # reset data\n",
    "        self.data = {\n",
    "            \"state\": [],\n",
    "            \"info\" : [],\n",
    "            \"action\": []}\n",
    "\n",
    "    def plot_safety(self,info,i):\n",
    "        info = np.array(info)\n",
    "        speed = info[:, 0]\n",
    "        gyro = info[:, 6]\n",
    "\n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot first graph in first subplot\n",
    "        ax1.plot(i, speed)\n",
    "        ax1.set_title('Speed over time')\n",
    "        \n",
    "        # Plot second graph in second subplot\n",
    "        ax2.plot(i, gyro)\n",
    "        ax2.set_title('Angular Velocity over time')\n",
    "\n",
    "        # Add labels and title for the entire figure\n",
    "        fig.suptitle('Safety')\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "    \n",
    "    def preprocess_state(states):\n",
    "        \"\"\" Preprocess the images (states) of the expert dataset before feeding them to agent \"\"\"\n",
    "        states_pp = np.copy(states)\n",
    "        \n",
    "        # Paint black over the sum of rewards\n",
    "        states_pp[:, 85:, :15] = [0.0, 0.0, 0.0]\n",
    "        \n",
    "        # Replace the colors defined bellow\n",
    "        def replace_color(old_color, new_color, states_pp):\n",
    "            mask = np.all(states_pp == old_color, axis=-1)\n",
    "            states_pp[mask] = new_color\n",
    "        \n",
    "        # Black bar\n",
    "        replace_color([0., 0., 0.], [120.0, 120.0, 120.0], states_pp)\n",
    "        #print(\"black bar replaced\")\n",
    "        # Road\n",
    "        new_road_color = [102.0, 102.0, 102.0]\n",
    "        replace_color([102., 102., 102.], new_road_color, states_pp)\n",
    "        replace_color([105., 105., 105.], new_road_color, states_pp)\n",
    "        replace_color([107., 107., 107.], new_road_color, states_pp)\n",
    "        #print(\"road replaced\")\n",
    "        # Curbs\n",
    "        replace_color([255., 0., 0.], new_road_color, states_pp)\n",
    "        replace_color([255., 255., 255.], new_road_color, states_pp)\n",
    "        #print(\"curb replaced\")\n",
    "\n",
    "        # Grass\n",
    "        #new_grass_color = [0.0, 0.0, 0.0]\n",
    "        new_grass_color = [102., 229., 102.]\n",
    "        replace_color([102., 229., 102.], new_grass_color, states_pp)\n",
    "        replace_color([102., 204., 102.], new_grass_color, states_pp)\n",
    "        #print(\"grass replaced\")\n",
    "        # Float RGB represenattion\n",
    "        #states_pp /= 255.\n",
    "\n",
    "        # Converting to gray scale\n",
    "        states_pp = rgb2gray(states_pp)\n",
    "\n",
    "        return states_pp\n",
    "    \n",
    "class Simulation:\n",
    "    #env = gym.make('CarRacing-v0').unwrapped\n",
    "    env = CarRacing()\n",
    "    user_input = User_Input()\n",
    "    current_state = np.ndarray((96, 96, 3))\n",
    "    data = Data()\n",
    "    step = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initialize window\n",
    "        self.current_state = self.env.reset()\n",
    "        # assign key action\n",
    "        self.env.viewer.window.on_key_press     = self.user_input.on_key_press\n",
    "        self.env.viewer.window.on_key_release   = self.user_input.on_key_release\n",
    "\n",
    "    def run_simulation(self):\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # act                \n",
    "            next_state, reward, done, info = self.env.step(self.user_input.get_action())\n",
    "            \n",
    "            # render\n",
    "            isopen = self.env.render()\n",
    "            self.step += 1\n",
    "            if self.step % 1000 == 0:\n",
    "                print(f'step number has been {self.step}')\n",
    "                \n",
    "            # update control\n",
    "            self.user_input.check_arrow_keys()\n",
    "\n",
    "            # ---------- ACTIONS ----------\n",
    "            # close window \n",
    "            if (self.user_input.escape_pressed()) : \n",
    "                self.env.close()\n",
    "                break\n",
    "\n",
    "            # record data\n",
    "            if (self.user_input.record_pressed()):\n",
    "                self.data.record(self.current_state, self.env.read_info(), self.user_input.get_action())\n",
    "                #print(self.user_input.get_action())\n",
    "            if (self.user_input.save_pressed()):\n",
    "                self.data.save()\n",
    "                self.user_input.on_save()\n",
    "\n",
    "\n",
    "            # --------- NEXT LOOP ---------\n",
    "            self.current_state = next_state\n",
    "            if done or self.user_input.reset==True or isopen == False:\n",
    "                self.env.reset()\n",
    "                self.user_input.reset = False\n",
    "    \n",
    "    def test_simulation(self, model):\n",
    "        all_info = []\n",
    "        frames = []\n",
    "        while True:\n",
    "            # Define a state to test the model on\n",
    "            state = preprocess_state(self.current_state)\n",
    "            info = self.env.read_info()\n",
    "            \n",
    "            all_info.append(info)\n",
    "            frames.append(self.step)\n",
    "\n",
    "            # Wrap the state and info in Tensors and add a batch dimension\n",
    "            state = torch.tensor(state).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
    "            info = torch.tensor(info).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "\n",
    "            # Pass the state and info through the model and get the output action\n",
    "            action = model((state, info))\n",
    "            # Convert tensor to tuple\n",
    "            action = tuple(action.squeeze().tolist())\n",
    "            \n",
    "            # if self.step < 200:\n",
    "            #     action = list(action)\n",
    "            #     action[1] = 1\n",
    "            #     action= tuple(action)\n",
    "\n",
    "              \n",
    "            #print(f\"output action is: {action}\")\n",
    "            \n",
    "            # act                \n",
    "            next_state, reward, done, i = self.env.step(action)\n",
    "            \n",
    "            # render\n",
    "            isopen = self.env.render()\n",
    "            self.step += 1\n",
    "            if self.step % 1000 == 0:\n",
    "                print(f'step number has been {self.step}')\n",
    "                \n",
    "            # update control\n",
    "            self.user_input.check_arrow_keys()\n",
    "\n",
    "            # ---------- ACTIONS ----------\n",
    "            # close window \n",
    "            if (self.user_input.escape_pressed()) : \n",
    "                self.env.close()\n",
    "                break\n",
    "\n",
    "            # --------- NEXT LOOP ---------\n",
    "            self.current_state = next_state\n",
    "            if done or self.user_input.reset==True or isopen == False:\n",
    "                self.step = 0\n",
    "                self.env.reset()\n",
    "                self.user_input.reset = False\n",
    "        self.data.plot_safety(all_info,frames)\n",
    "\n",
    "    def dagger(self,model):\n",
    "        max_timesteps=1000\n",
    "        n_test_episodes = 10\n",
    "        episode_reward = 0\n",
    "        episode_rewards = []#To record the average reward\n",
    "        good_expert=0\n",
    "\n",
    "        for i in range(n_test_episodes):\n",
    "            self.data.data[\"state\"]=[]\n",
    "            self.data.data[\"info\"]=[]\n",
    "            self.data.data[\"action\"]=[]\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            for _ in range(max_timesteps):\n",
    "                # Define a state to test the model on\n",
    "                state = preprocess_state(self.current_state)\n",
    "                info = self.env.read_info()#\n",
    "\n",
    "                # Wrap the state and info in Tensors and add a batch dimension\n",
    "                state = torch.tensor(state).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
    "                info = torch.tensor(info).unsqueeze(0).to(torch.float32)\n",
    "\n",
    "                # Pass the state and info through the model and get the output action\n",
    "                action = model((state, info))\n",
    "                # Convert tensor to tuple\n",
    "                action = tuple(action.squeeze().tolist())\n",
    "                \n",
    "                # act                \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                # record\n",
    "                episode_reward += reward\n",
    "                self.data.record(self.current_state, info, action)\n",
    "\n",
    "                \n",
    "                # render\n",
    "                isopen = self.env.render()\n",
    "                self.current_state = next_state\n",
    "                self.step += 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # ---------- ACTIONS ----------\n",
    "                # close window \n",
    "                if (self.user_input.escape_pressed()) : \n",
    "                    self.env.close()\n",
    "                    break\n",
    "\n",
    "                # --------- NEXT LOOP ---------\n",
    "                if self.user_input.reset==True or isopen == False:\n",
    "                    self.env.reset()\n",
    "                    self.user_input.reset = False\n",
    "            if (i+1) % 10 ==0:\n",
    "                print(f'Episode {i+1}') \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            if episode_reward > 1000:\n",
    "                good_expert+=1\n",
    "                print(f'GOOD EXPERT with reward {episode_reward}') \n",
    "                self.data.save()\n",
    "        self.env.close()\n",
    "        average = sum(episode_rewards) / len(episode_rewards)\n",
    "        print(f\"-------------AVERAGE REWARD: {average}\")\n",
    "        print(f\"-------------No.GOOD EXPERT: {good_expert} out of {n_test_episodes} experts\")  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1287..1613 -> 326-tiles track\n",
      "Track generation: 1108..1389 -> 281-tiles track\n",
      "start recording\n",
      "step number has been 1000\n",
      "Track generation: 1178..1478 -> 300-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1164..1459 -> 295-tiles track\n",
      "step number has been 2000\n",
      "step number has been 3000\n",
      "Track generation: 1129..1415 -> 286-tiles track\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-38-09.pkl.gzip\n",
      "Track generation: 1066..1344 -> 278-tiles track\n",
      "start recording\n",
      "step number has been 4000\n",
      "Track generation: 1083..1362 -> 279-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1163..1458 -> 295-tiles track\n",
      "step number has been 5000\n",
      "Track generation: 1177..1484 -> 307-tiles track\n",
      "step number has been 6000\n",
      "step number has been 7000\n",
      "Track generation: 1275..1598 -> 323-tiles track\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-39-31.pkl.gzip\n",
      "Track generation: 1121..1406 -> 285-tiles track\n",
      "start recording\n",
      "step number has been 8000\n",
      "step number has been 9000\n",
      "Track generation: 956..1203 -> 247-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1125..1415 -> 290-tiles track\n",
      "step number has been 10000\n",
      "step number has been 11000\n",
      "Track generation: 1059..1328 -> 269-tiles track\n",
      "step number has been 12000\n",
      "step number has been 13000\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-41-12.pkl.gzip\n",
      "Track generation: 1195..1498 -> 303-tiles track\n",
      "start recording\n",
      "step number has been 14000\n",
      "step number has been 15000\n",
      "Track generation: 1100..1379 -> 279-tiles track\n",
      "step number has been 16000\n",
      "step number has been 17000\n",
      "Track generation: 1076..1349 -> 273-tiles track\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-42-36.pkl.gzip\n",
      "Track generation: 1271..1602 -> 331-tiles track\n",
      "start recording\n",
      "step number has been 18000\n",
      "step number has been 19000\n",
      "Track generation: 1079..1352 -> 273-tiles track\n",
      "step number has been 20000\n",
      "step number has been 21000\n",
      "step number has been 22000\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-44-09.pkl.gzip\n",
      "Track generation: 1177..1476 -> 299-tiles track\n",
      "start recording\n",
      "step number has been 23000\n",
      "Track generation: 1240..1561 -> 321-tiles track\n",
      "step number has been 24000\n",
      "step number has been 25000\n",
      "Track generation: 1082..1356 -> 274-tiles track\n",
      "step number has been 26000\n",
      "Track generation: 1152..1444 -> 292-tiles track\n",
      "step number has been 27000\n",
      "Track generation: 1110..1391 -> 281-tiles track\n",
      "step number has been 28000\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-46-02.pkl.gzip\n",
      "Track generation: 1266..1587 -> 321-tiles track\n",
      "start recording\n",
      "step number has been 29000\n",
      "Track generation: 944..1187 -> 243-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1123..1408 -> 285-tiles track\n",
      "step number has been 30000\n",
      "step number has been 31000\n",
      "step number has been 32000\n",
      "Track generation: 1203..1508 -> 305-tiles track\n",
      "step number has been 33000\n",
      "step number has been 34000\n",
      "step number has been 35000\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-48-19.pkl.gzip\n",
      "Track generation: 1099..1378 -> 279-tiles track\n",
      "start recording\n",
      "step number has been 36000\n",
      "Track generation: 911..1147 -> 236-tiles track\n",
      "step number has been 37000\n",
      "Track generation: 1248..1564 -> 316-tiles track\n",
      "step number has been 38000\n",
      "step number has been 39000\n",
      "Track generation: 1191..1499 -> 308-tiles track\n",
      "step number has been 40000\n",
      "Track generation: 1227..1538 -> 311-tiles track\n",
      "step number has been 41000\n",
      "step number has been 42000\n",
      "Track generation: 1151..1449 -> 298-tiles track\n",
      "step number has been 43000\n",
      "paused recording, start saving\n",
      "image processed\n",
      "saved data to ./data2/2023_03_09_00-50-53.pkl.gzip\n"
     ]
    }
   ],
   "source": [
    "simulation = Simulation()\n",
    "simulation.run_simulation()\n",
    "#simulation.test_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "class Classification:\n",
    "    actions= torch.tensor([\n",
    "        [0.0, 0.0, 0.0],  # STRAIGHT\n",
    "        [0.0, 1.0, 0.0],  # ACCELERATE\n",
    "        [1.0, 0.0, 0.0],  # RIGHT\n",
    "        [1.0, 0.0, 0.4],  # RIGHT_BRAKE\n",
    "        [0.0, 0.0, 0.4],  # BRAKE\n",
    "        [-1.0, 0.0, 0.4],  # LEFT_BRAKE\n",
    "        [-1.0, 0.0, 0.0],  # LEFT\n",
    "    ], dtype=torch.float32)\n",
    "    n_actions = actions.size()[0]\n",
    "\n",
    "    def action_arr2id(self,arr):\n",
    "        \"\"\" Converts action from the array format to an id (ranging from 0 to n_actions) \"\"\"\n",
    "        ids = torch.zeros(arr.shape[:-1], dtype=torch.int64)\n",
    "        for i in range(self.n_actions):\n",
    "            mask = torch.all(arr == self.actions[i], dim=-1)\n",
    "            ids[mask] = i\n",
    "        return ids\n",
    "\n",
    "    def action_id2arr(self, ids):\n",
    "        \"\"\" Converts action from id to array format (as understood by the environment) \"\"\"\n",
    "        return torch.stack([torch.tensor(self.actions[i], dtype=torch.float32) for i in ids])\n",
    "\n",
    "    def one_hot(self,labels):\n",
    "        \"\"\" One hot encodes a set of actions \"\"\"\n",
    "        one_hot_labels = torch.zeros(labels.shape + (self.n_actions,), dtype=torch.float32)\n",
    "        for c in range(self.n_actions):\n",
    "            one_hot_labels[labels == c, c] = 1.0\n",
    "        return one_hot_labels  \n",
    "\n",
    "    def unhot(self, one_hot_labels):\n",
    "        \"\"\" One hot DEcodes a set of actions \"\"\"\n",
    "        return torch.argmax(one_hot_labels, dim=1)\n",
    "\n",
    "    def transl_action_env2agent(self, acts):\n",
    "        \"\"\" Translate actions from environment's format to agent's format \"\"\"\n",
    "        act_ids = self.action_arr2id(acts)\n",
    "        return self.one_hot(act_ids)\n",
    "\n",
    "    def transl_action_agent2env(self, one_hot_labels):\n",
    "        \"\"\" Translate actions from agent's format to environment's format \"\"\"\n",
    "        ids = torch.argmax(one_hot_labels, dim=1).numpy()\n",
    "        return self.action_id2arr(ids)\n",
    "    def delete_invalid_actions(self, y):\n",
    "        \"\"\" Check if there is any forbidden actions in the expert database \"\"\"\n",
    "        inval_actions = [\n",
    "            [0.0, 1.0, 0.4],  # ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.4],  # RIGHT_ACCEL_BRAKE\n",
    "            [-1.0, 1.0, 0.4],  # LEFT_ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.0],  # RIGHT_ACCEL\n",
    "            [-1.0, 1.0, 0.0],  # LEFT_ACCEL\n",
    "        ]\n",
    "        y = np.array(y)  # Convert to NumPy array\n",
    "        ia_count = 0\n",
    "        ia_indices = []\n",
    "        for ia in inval_actions:\n",
    "            ia_indices += list(np.where(np.all(np.isclose(y, ia, rtol=1e-05, atol=1e-08), axis=1))[0])\n",
    "        ia_count += len(ia_indices)\n",
    "        if ia_count > 0:\n",
    "            print(\n",
    "                f'Removing {ia_count} invalid actions at indices: {ia_indices}'\n",
    "            )\n",
    "            y = np.delete(y, ia_indices, axis=0)\n",
    "        #return a clean action and dirty index\n",
    "        return y, ia_indices\n",
    "\n",
    "a = torch.tensor([\n",
    "    [0.0, 1.0, 0.4],  # ACCEL_BRAKE\n",
    "    [0.0, 0.0, 0.0],  # STRAIGHT\n",
    "    [0.0, 1.0, 0.0],  # ACCELERATE\n",
    "    [1.0, 0.0, 0.0],  # RIGHT\n",
    "    [1.0, 0.0, 0.4],  # RIGHT_BRAKE\n",
    "    [0.0, 0.0, 0.4],  # BRAKE\n",
    "    [-1.0, 0.0, 0.4],  # LEFT_BRAKE\n",
    "    [-1.0, 0.0, 0.0],  # LEFT\n",
    "], dtype=torch.float32)\n",
    "\n",
    "cla = Classification()\n",
    "y,i = cla.delete_invalid_actions(a)\n",
    "print(f'y={y}')\n",
    "print(f'i={i}')\n",
    "one=cla.transl_action_env2agent(a)\n",
    "print(one)\n",
    "a = cla.transl_action_agent2env(one)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed invalid actions: 3847\n",
      "Total number of data points: 88928\n",
      "Number of [0, 0, 0] actions: 53128\n",
      "Length of train_dataloader: 312\n",
      "Length of val_dataloader: 34\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    #calculate the number of zero action among all actions\n",
    "    \n",
    "    @staticmethod\n",
    "    def num_zero(action):\n",
    "        if (action == np.array([0,0,0])).all():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def delete_invalid_actions(self, action):\n",
    "        \"\"\" Check if there is any forbidden actions in the expert database \"\"\"\n",
    "        inval_actions = [\n",
    "            [0.0, 1.0, 0.4],  # ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.4],  # RIGHT_ACCEL_BRAKE\n",
    "            [-1.0, 1.0, 0.4],  # LEFT_ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.0],  # RIGHT_ACCEL\n",
    "            [-1.0, 1.0, 0.0],  # LEFT_ACCEL\n",
    "        ]\n",
    "        for inval_action in inval_actions:\n",
    "            if all(action == inval_action):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def __init__(self, directory_path, batch_size, train_frac):\n",
    "        self.paths = []\n",
    "        self.data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.directory_path = directory_path\n",
    "        self.train_frac = train_frac\n",
    "        self.zerocount = 0\n",
    "        self.invalidcount = 0\n",
    "        \n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".pkl.gzip\"):\n",
    "                with gzip.open(os.path.join(self.directory_path, filename), 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    for state, info, action in zip(data[\"state\"], data[\"info\"], data[\"action\"]):\n",
    "                        if self.num_zero(action):\n",
    "                            self.zerocount += 1\n",
    "                        if self.delete_invalid_actions(action):\n",
    "                            self.data.append((state, info, action))\n",
    "                        else:\n",
    "                            self.invalidcount += 1\n",
    "\n",
    "        random.shuffle(self.data)\n",
    "    \n",
    "        #split the data\n",
    "        num_train = int(len(self.data) * train_frac)\n",
    "        self.train_data = self.data[:num_train]\n",
    "        self.val_data = self.data[num_train:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            state, info, action = self.train_data[index]\n",
    "        else:\n",
    "            state, info, action = self.val_data[index - len(self.train_data)]\n",
    "\n",
    "        return torch.tensor(state), torch.tensor(info), torch.tensor(action)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.batch_size == 1:\n",
    "            return len(self.train_data) + len(self.val_data)\n",
    "        else:\n",
    "            return len(self.train_data) // self.batch_size + len(self.val_data) // self.batch_size\n",
    "   \n",
    "# Create dataset\n",
    "dataset = MyDataset(directory_path=os.path.join(os.getcwd(), \"data2\"), batch_size=256, train_frac=0.9)\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(range(len(dataset.train_data)))\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(range(len(dataset.train_data), len(dataset.data)))\n",
    "\n",
    "#Create Dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=256, drop_last=True, num_workers=4, pin_memory=True, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=256, drop_last=True, num_workers=4, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "print(\"number of removed invalid actions:\", dataset.invalidcount)\n",
    "print(\"Total number of data points:\", len(dataset.train_data) + len(dataset.val_data))\n",
    "print(\"Number of [0, 0, 0] actions:\", dataset.zerocount)\n",
    "\n",
    "print(\"Length of train_dataloader:\", len(train_dataloader))\n",
    "print(\"Length of val_dataloader:\", len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "312\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec2d195f68380fc51145559d6f50b89f553571b7ee7c2564da641501c7d14910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
