{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import jiahao\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, array_size, n_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.array_size = array_size\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Calculate the number of units needed in the first fully connected layer\n",
    "        # based on the output shape of the convolutional layers\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros((1, 1, array_size, array_size))\n",
    "            test_output = self.conv_layers(test_input)\n",
    "            fc_input_size = test_output.shape[1] + 7\n",
    "        # +7 because it will be cacantinate by 7\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(fc_input_size, 128),\n",
    "            nn.Linear(128, 7)  # change n_classes to 7\n",
    "            #nn.Softmax(dim=1)  # add softmax layer\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        image_input, info_input = inputs\n",
    "        image_input = image_input.unsqueeze(1)  # add a channel dimension to the image input\n",
    "        x = self.conv_layers(image_input)\n",
    "        x = torch.cat((x, info_input), dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed invalid actions: 3847\n",
      "Total number of data points: 88928\n",
      "Number of [0, 0, 0] actions: 53128\n",
      "Length of train_dataloader: 277\n",
      "Length of val_dataloader: 69\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    #calculate the number of zero action among all actions\n",
    "    \n",
    "    @staticmethod\n",
    "    def num_zero(action):\n",
    "        if (action == np.array([0,0,0])).all():\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def delete_invalid_actions(self, action):\n",
    "        \"\"\" Check if there is any forbidden actions in the expert database \"\"\"\n",
    "        inval_actions = [\n",
    "            [0.0, 1.0, 0.4],  # ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.4],  # RIGHT_ACCEL_BRAKE\n",
    "            [-1.0, 1.0, 0.4],  # LEFT_ACCEL_BRAKE\n",
    "            [1.0, 1.0, 0.0],  # RIGHT_ACCEL\n",
    "            [-1.0, 1.0, 0.0],  # LEFT_ACCEL\n",
    "        ]\n",
    "        for inval_action in inval_actions:\n",
    "            if all(action == inval_action):\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def __init__(self, directory_path, batch_size, train_frac):\n",
    "        self.paths = []\n",
    "        self.data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.directory_path = directory_path\n",
    "        self.train_frac = train_frac\n",
    "        self.zerocount = 0\n",
    "        self.invalidcount = 0\n",
    "        \n",
    "        for filename in os.listdir(directory_path):\n",
    "            if filename.endswith(\".pkl.gzip\"):\n",
    "                with gzip.open(os.path.join(self.directory_path, filename), 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    for state, info, action in zip(data[\"state\"], data[\"info\"], data[\"action\"]):\n",
    "                        if self.num_zero(action):\n",
    "                            self.zerocount += 1\n",
    "                        if self.delete_invalid_actions(action):\n",
    "                            self.data.append((state, info, action))\n",
    "                        else:\n",
    "                            self.invalidcount += 1\n",
    "\n",
    "        random.shuffle(self.data)\n",
    "    \n",
    "        #split the data\n",
    "        num_train = int(len(self.data) * train_frac)\n",
    "        self.train_data = self.data[:num_train]\n",
    "        self.val_data = self.data[num_train:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < len(self.train_data):\n",
    "            state, info, action = self.train_data[index]\n",
    "        else:\n",
    "            state, info, action = self.val_data[index - len(self.train_data)]\n",
    "\n",
    "        return torch.tensor(state), torch.tensor(info), torch.tensor(action)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.batch_size == 1:\n",
    "            return len(self.train_data) + len(self.val_data)\n",
    "        else:\n",
    "            return len(self.train_data) // self.batch_size + len(self.val_data) // self.batch_size\n",
    "   \n",
    "# Create dataset\n",
    "dataset = MyDataset(directory_path=os.path.join(os.getcwd(), \"data2\"), batch_size=256, train_frac=0.8)\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(range(len(dataset.train_data)))\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(range(len(dataset.train_data), len(dataset.data)))\n",
    "\n",
    "#Create Dataloader\n",
    "train_dataloader = DataLoader(dataset, batch_size=256, drop_last=True, num_workers=4, pin_memory=True, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=256, drop_last=True, num_workers=4, pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "print(\"number of removed invalid actions:\", dataset.invalidcount)\n",
    "print(\"Total number of data points:\", len(dataset.train_data) + len(dataset.val_data))\n",
    "print(\"Number of [0, 0, 0] actions:\", dataset.zerocount)\n",
    "\n",
    "print(\"Length of train_dataloader:\", len(train_dataloader))\n",
    "print(\"Length of val_dataloader:\", len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n",
      "312\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 1.208989 | Val Loss: 0.786369\n",
      "Epoch 2 | Train Loss: 0.755425 | Val Loss: 0.813207\n",
      "Epoch 3 | Train Loss: 0.707091 | Val Loss: 0.684687\n",
      "Epoch 4 | Train Loss: 0.679580 | Val Loss: 0.725752\n",
      "Epoch 5 | Train Loss: 0.661550 | Val Loss: 0.645465\n",
      "Epoch 6 | Train Loss: 0.652827 | Val Loss: 0.667686\n",
      "Epoch 7 | Train Loss: 0.642637 | Val Loss: 0.657104\n",
      "Epoch 8 | Train Loss: 0.627416 | Val Loss: 0.682440\n",
      "Epoch 9 | Train Loss: 0.622430 | Val Loss: 0.628002\n",
      "Epoch 10 | Train Loss: 0.612305 | Val Loss: 0.656851\n",
      "Epoch 11 | Train Loss: 0.607653 | Val Loss: 0.648620\n",
      "Epoch 12 | Train Loss: 0.598278 | Val Loss: 0.626244\n",
      "Epoch 13 | Train Loss: 0.591399 | Val Loss: 0.654907\n",
      "Epoch 14 | Train Loss: 0.583890 | Val Loss: 0.596962\n",
      "Epoch 15 | Train Loss: 0.576886 | Val Loss: 0.617552\n",
      "Epoch 16 | Train Loss: 0.568739 | Val Loss: 0.626408\n",
      "Epoch 17 | Train Loss: 0.564495 | Val Loss: 0.624835\n",
      "Epoch 18 | Train Loss: 0.556438 | Val Loss: 0.726829\n",
      "Epoch 19 | Train Loss: 0.552250 | Val Loss: 0.581525\n",
      "Epoch 20 | Train Loss: 0.548706 | Val Loss: 0.736757\n",
      "Epoch 21 | Train Loss: 0.537732 | Val Loss: 0.754234\n",
      "Epoch 22 | Train Loss: 0.532254 | Val Loss: 0.553776\n",
      "Epoch 23 | Train Loss: 0.532309 | Val Loss: 0.585381\n",
      "Epoch 24 | Train Loss: 0.524266 | Val Loss: 0.534903\n",
      "Epoch 25 | Train Loss: 0.524385 | Val Loss: 0.721969\n",
      "Epoch 26 | Train Loss: 0.511733 | Val Loss: 0.561066\n",
      "Epoch 27 | Train Loss: 0.517566 | Val Loss: 0.594422\n",
      "Epoch 28 | Train Loss: 0.514615 | Val Loss: 0.842062\n",
      "Epoch 29 | Train Loss: 0.507655 | Val Loss: 0.574573\n",
      "Epoch 30 | Train Loss: 0.503428 | Val Loss: 0.590491\n",
      "Epoch 31 | Train Loss: 0.500011 | Val Loss: 0.536862\n",
      "Epoch 32 | Train Loss: 0.501248 | Val Loss: 0.546470\n",
      "Epoch 33 | Train Loss: 0.494025 | Val Loss: 0.575530\n",
      "Epoch 34 | Train Loss: 0.494710 | Val Loss: 0.534211\n",
      "Epoch 35 | Train Loss: 0.491331 | Val Loss: 0.667176\n",
      "Epoch 36 | Train Loss: 0.492484 | Val Loss: 0.533526\n",
      "Epoch 37 | Train Loss: 0.490802 | Val Loss: 0.537219\n",
      "Epoch 38 | Train Loss: 0.486042 | Val Loss: 0.527903\n",
      "Epoch 39 | Train Loss: 0.484937 | Val Loss: 0.529146\n",
      "Epoch 40 | Train Loss: 0.483582 | Val Loss: 0.560450\n",
      "Epoch 41 | Train Loss: 0.479469 | Val Loss: 0.535268\n",
      "Epoch 42 | Train Loss: 0.483492 | Val Loss: 0.565715\n",
      "Epoch 43 | Train Loss: 0.475674 | Val Loss: 0.529329\n",
      "Epoch 44 | Train Loss: 0.478196 | Val Loss: 0.590058\n",
      "Epoch 45 | Train Loss: 0.475745 | Val Loss: 0.559684\n",
      "Epoch 46 | Train Loss: 0.473364 | Val Loss: 0.564120\n",
      "Epoch 47 | Train Loss: 0.470306 | Val Loss: 0.609462\n",
      "Epoch 48 | Train Loss: 0.471544 | Val Loss: 0.741128\n",
      "Epoch 49 | Train Loss: 0.470729 | Val Loss: 0.578338\n",
      "Epoch 50 | Train Loss: 0.472015 | Val Loss: 0.654081\n",
      "Epoch 51 | Train Loss: 0.464563 | Val Loss: 0.567786\n",
      "Epoch 52 | Train Loss: 0.467803 | Val Loss: 0.586460\n",
      "Epoch 53 | Train Loss: 0.469080 | Val Loss: 0.547728\n",
      "Epoch 54 | Train Loss: 0.474571 | Val Loss: 0.553469\n",
      "Epoch 55 | Train Loss: 0.467560 | Val Loss: 0.557408\n",
      "Epoch 56 | Train Loss: 0.470351 | Val Loss: 0.530811\n",
      "Epoch 57 | Train Loss: 0.468429 | Val Loss: 0.528260\n",
      "Epoch 58 | Train Loss: 0.465766 | Val Loss: 0.579723\n",
      "Epoch 59 | Train Loss: 0.461920 | Val Loss: 0.584386\n",
      "Epoch 60 | Train Loss: 0.463611 | Val Loss: 0.565991\n",
      "Epoch 61 | Train Loss: 0.466480 | Val Loss: 0.517076\n",
      "Epoch 62 | Train Loss: 0.464319 | Val Loss: 0.570161\n",
      "Epoch 63 | Train Loss: 0.460035 | Val Loss: 0.522471\n",
      "Epoch 64 | Train Loss: 0.461072 | Val Loss: 0.555249\n",
      "Epoch 65 | Train Loss: 0.456485 | Val Loss: 0.627877\n",
      "Epoch 66 | Train Loss: 0.462550 | Val Loss: 0.608386\n",
      "Epoch 67 | Train Loss: 0.460735 | Val Loss: 0.529263\n",
      "Epoch 68 | Train Loss: 0.458783 | Val Loss: 0.524936\n",
      "Epoch 69 | Train Loss: 0.457941 | Val Loss: 0.530551\n",
      "Epoch 70 | Train Loss: 0.456654 | Val Loss: 0.521929\n",
      "Epoch 71 | Train Loss: 0.457879 | Val Loss: 0.514898\n",
      "Epoch 72 | Train Loss: 0.459002 | Val Loss: 0.549788\n",
      "Epoch 73 | Train Loss: 0.453351 | Val Loss: 0.526282\n",
      "Epoch 74 | Train Loss: 0.457467 | Val Loss: 0.556291\n",
      "Epoch 75 | Train Loss: 0.452359 | Val Loss: 0.514354\n",
      "Epoch 76 | Train Loss: 0.452236 | Val Loss: 0.527589\n",
      "Epoch 77 | Train Loss: 0.457429 | Val Loss: 0.518646\n",
      "Epoch 78 | Train Loss: 0.452362 | Val Loss: 0.586686\n",
      "Epoch 79 | Train Loss: 0.453074 | Val Loss: 0.665068\n",
      "Epoch 80 | Train Loss: 0.450476 | Val Loss: 0.533421\n",
      "Epoch 81 | Train Loss: 0.455327 | Val Loss: 0.551171\n",
      "Epoch 82 | Train Loss: 0.453088 | Val Loss: 0.670603\n",
      "Epoch 83 | Train Loss: 0.446813 | Val Loss: 0.536945\n",
      "Epoch 84 | Train Loss: 0.454505 | Val Loss: 0.543462\n",
      "Epoch 85 | Train Loss: 0.449301 | Val Loss: 0.605023\n",
      "Epoch 86 | Train Loss: 0.447887 | Val Loss: 0.522246\n",
      "Epoch 87 | Train Loss: 0.447702 | Val Loss: 0.584336\n",
      "Epoch 88 | Train Loss: 0.454073 | Val Loss: 0.525272\n",
      "Epoch 89 | Train Loss: 0.447411 | Val Loss: 0.540950\n",
      "Epoch 90 | Train Loss: 0.447624 | Val Loss: 0.558999\n",
      "Epoch 91 | Train Loss: 0.446252 | Val Loss: 0.791855\n",
      "Epoch 92 | Train Loss: 0.449597 | Val Loss: 0.552806\n",
      "Epoch 93 | Train Loss: 0.444837 | Val Loss: 0.545948\n",
      "Epoch 94 | Train Loss: 0.444852 | Val Loss: 0.539889\n",
      "Epoch 95 | Train Loss: 0.451433 | Val Loss: 0.535336\n",
      "Epoch 96 | Train Loss: 0.445296 | Val Loss: 0.549551\n",
      "Epoch 97 | Train Loss: 0.447652 | Val Loss: 0.539291\n",
      "Epoch 98 | Train Loss: 0.448223 | Val Loss: 0.553828\n",
      "Epoch 99 | Train Loss: 0.445186 | Val Loss: 0.547180\n",
      "Epoch 100 | Train Loss: 0.444497 | Val Loss: 0.549527\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "###########TEST\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "array_size = 96 #width and height of the image\n",
    "\n",
    "model = NeuralNetwork(array_size=array_size, n_classes=7).cuda()\n",
    "classification = jiahao.Classification()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# Use the CE loss function for training\n",
    "def train_epoch(model, optimizer, dataloader, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    for state_batch, info_batch, action_batch in dataloader:\n",
    "        state_batch = state_batch.to(device)\n",
    "        info_batch = info_batch.to(device)\n",
    "        action_batch = action_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        action_label = classification.transl_action_env2agent(action_batch, device)\n",
    "        \n",
    "        output = model((state_batch.float(), info_batch.float()))\n",
    "\n",
    "        loss = criterion(output, action_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= num_batches\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "# Use the CE loss function for validation\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    with torch.no_grad():\n",
    "        for state_batch, info_batch, action_batch in dataloader:\n",
    "            state_batch = state_batch.to(device)\n",
    "            info_batch = info_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            output = model((state_batch.float(), info_batch.float()))\n",
    "\n",
    "            \n",
    "            action_label = classification.transl_action_env2agent(action_batch, device)\n",
    "            #print(action_batch[0])\n",
    "            loss = criterion(output, action_label)\n",
    "            #print(f\"loss={loss}\")\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= num_batches\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "# Initialize a SummaryWriter object\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, optimizer, train_dataloader, criterion)\n",
    "    val_loss = validate(model, val_dataloader, criterion)\n",
    "    \n",
    "    # Record the train and validation losses in Tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "if not os.path.exists(\"class_model\"):\n",
    "    os.makedirs(\"class_model\")\n",
    "PATH = \"class_model/my_model.pt\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print(\"model saved\")\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-13 23:02:46.473690: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, state, info, action):\n",
    "        self.paths = [\"\"]\n",
    "        self.img = state\n",
    "        self.input = info\n",
    "        self.targets = action\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.img[index]\n",
    "        inp = self.input[index]\n",
    "\n",
    "        y = self.targets[index]\n",
    "        return img, inp, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data\n",
    "state, action, info = read_data()\n",
    "#state = jiahao.preprocess_state(state)\n",
    "classification = jiahao.Classification()\n",
    "\n",
    "action, index = classification.delete_invalid_actions(action)\n",
    "num_zero(action)\n",
    "state = np.delete(state, index, axis=0)\n",
    "info = np.delete(info, index, axis=0)\n",
    "\n",
    "#covert numpy array into tensor\n",
    "state = torch.from_numpy(state).to(torch.float32)\n",
    "info   = torch.from_numpy(info).to(torch.float32)\n",
    "action = torch.from_numpy(action).to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state, action, info = read_data()\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# create some large data to display\n",
    "data = action\n",
    "\n",
    "# create a HTML table to display the data\n",
    "table = \"<table>{}</table>\"\n",
    "rows = \"\"\n",
    "for i, d in enumerate(data):\n",
    "    rows += \"<tr><td>{}</td><td>{}</td></tr>\".format(i, d)\n",
    "html = table.format(rows)\n",
    "\n",
    "# display the HTML table in a scrollable output area\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the dataset into train set and validation set\n",
    "frac = 0.1\n",
    "split = int((1-frac) * len(action))\n",
    "num_epochs = 30\n",
    "\n",
    "train_dataset = MyDataset(state[:split], info[:split], action[:split])\n",
    "validation_dataset = MyDataset(state[split:], info[split:], action[split:])\n",
    "# Initialize training parameters\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "array_size = 96 #width and height of the image\n",
    "n_classes = action.shape[1]\n",
    "model = NeuralNetwork(array_size=array_size, n_classes=n_classes).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# Use the CE loss function for training\n",
    "def train_epoch(model, optimizer, dataloader, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for state_batch, info_batch, action_batch in dataloader:\n",
    "        state_batch = state_batch.to(device)\n",
    "        info_batch = info_batch.to(device)\n",
    "        action_batch = action_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        action_label = classification.transl_action_env2agent(action_batch, device)\n",
    "        \n",
    "        output = model((state_batch, info_batch)) \n",
    "        loss = criterion(output, action_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# Use the CE loss function for validation\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for state_batch, info_batch, action_batch in dataloader:\n",
    "            state_batch = state_batch.to(device)\n",
    "            info_batch = info_batch.to(device)\n",
    "            action_batch = action_batch.to(device)\n",
    "            output = model((state_batch, info_batch))\n",
    "            \n",
    "            action_label = classification.transl_action_env2agent(action_batch, device)\n",
    "            #print(action_batch[0])\n",
    "            loss = criterion(output, action_label)\n",
    "            #print(f\"loss={loss}\")\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Initialize a SummaryWriter object\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    val_loss = validate(model, val_dataloader, criterion)\n",
    "    train_loss = train_epoch(model, optimizer, train_dataloader, criterion)\n",
    "    # Record the train and validation losses in Tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validation', val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "if not os.path.exists(\"class_model\"):\n",
    "    os.makedirs(\"class_model\")\n",
    "PATH = \"class_model/my_model.pt\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "print(\"model saved\")\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec2d195f68380fc51145559d6f50b89f553571b7ee7c2564da641501c7d14910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
